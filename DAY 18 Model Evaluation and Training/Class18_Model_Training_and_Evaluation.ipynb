{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c4ed77",
   "metadata": {},
   "source": [
    "# Class 18 â€“ Model Training and Evaluation\n",
    "### Duration: 2 Hours\n",
    "**Objective:** Learn to train, evaluate, validate, and tune machine learning models using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a80d1",
   "metadata": {},
   "source": [
    "## 1. Introduction to Model Training\n",
    "Model training is the process where a machine learning algorithm learns from data.\n",
    "\n",
    "### Real-life Analogy:\n",
    "Imagine you're teaching a child to identify fruits. You show them many apples and bananas and tell them their names. Over time, the child learns to differentiate them â€” this is *training*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe937228",
   "metadata": {},
   "source": [
    "### Steps in Model Training\n",
    "1. **Data Preprocessing**: Clean and prepare data.\n",
    "2. **Model Selection**: Choose an algorithm.\n",
    "3. **Training**: Fit the model to the data.\n",
    "4. **Evaluation**: Check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8cafb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911628e9",
   "metadata": {},
   "source": [
    "We will use the famous **Iris dataset**, which contains measurements of iris flowers from three different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a9cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d200e9",
   "metadata": {},
   "source": [
    "We divide the dataset into a **training set** (80%) and a **testing set** (20%) to simulate how the model performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f8400",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metrics\n",
    "We will now evaluate how well our model performs using different metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91addb3",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy is the ratio of correct predictions to the total predictions.\n",
    "\n",
    "\\[ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867dc99",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "- **Precision**: Out of all predicted positives, how many were truly positive?\n",
    "  \\[ Precision = \\frac{TP}{TP + FP} \\]\n",
    "\n",
    "  Important for situations where false positives are costly (e.g., spam detection).\n",
    "\n",
    "\n",
    "- **Recall**: Out of all actual positives, how many did we predict correctly?\n",
    "  \\[ Recall = \\frac{TP}{TP + FN} \\]\n",
    "  \n",
    "  Important when missing a positive class (false negatives) is more costly (e.g., medical diagnoses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103e6ab",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "F1-score combines precision and recall using harmonic mean:\n",
    "\n",
    "\\[ F1 = 2 \\times \\frac{Precision \\cdot Recall}{Precision + Recall} \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91552af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision (macro): 1.0\n",
      "Recall (macro): 1.0\n",
      "F1 Score (macro): 1.0\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Train a simple SVM model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision (macro):\", precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall (macro):\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1 Score (macro):\", f1_score(y_test, y_pred, average='macro'))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9eb2b",
   "metadata": {},
   "source": [
    "The confusion matrix shows how many instances were correctly or incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66e8b9",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation\n",
    "\n",
    "Cross-validation is a technique used to assess the generalizability of a model.\n",
    "\n",
    "\n",
    "It splits the data into multiple subsets (folds) and trains the model on each fold while testing it on the remaining folds.\n",
    "\n",
    "\n",
    "K-Fold Cross-Validation:\n",
    "The dataset is split into K folds (e.g., 5 or 10).\n",
    "\n",
    "\n",
    "The model is trained on K-1 folds and tested on the remaining fold.\n",
    "\n",
    "\n",
    "This process is repeated K times, and the final performance metric is averaged.\n",
    "\n",
    "\n",
    "Why Use Cross-Validation?\n",
    "Helps avoid overfitting by using different data subsets.\n",
    "\n",
    "\n",
    "Provides a better estimate of model performance on unseen data.\n",
    "\n",
    "Cross-validation is a robust way to evaluate model performance. It avoids overfitting by splitting data into multiple train/test folds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62397b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.96666667 0.96666667 0.96666667 0.93333333 1.        ]\n",
      "Average CV Score: 0.9666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Average CV Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810b407",
   "metadata": {},
   "source": [
    "This gives us a more reliable estimate of model performance across different data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33d069",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning - GridSearchCV\n",
    "Let's improve the model performance by tuning its parameters.\n",
    "\n",
    "What Are Hyperparameters?\n",
    "\n",
    "Hyperparameters are parameters that are set before the learning process begins, such as the learning rate, number of trees in a Random Forest, or the C parameter in SVM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38106849",
   "metadata": {},
   "source": [
    "GridSearchCV:\n",
    "\n",
    "A technique to search for the best combination of hyperparameters using an exhaustive search over a predefined grid of hyperparameters.\n",
    "\n",
    "Example:\n",
    "\n",
    "Hyperparameter Grid: {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e2b32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'kernel': 'linear'}\n",
      "Best Score: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Define the model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Step 3: Create GridSearchCV object\n",
    "grid = GridSearchCV(svm_model, param_grid, cv=5)\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3ec75",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning - RandomizedSearchCV\n",
    "\n",
    "A method that randomly samples hyperparameters from a given distribution.\n",
    "\n",
    "\n",
    "This is less computationally expensive than GridSearchCV.\n",
    "\n",
    "A quicker alternative is to randomly sample parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae2ca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Random): {'C': 4, 'kernel': 'linear'}\n",
      "Best Score (Random): 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Step 1: Define the model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define hyperparameter distribution\n",
    "param_dist = {'C': randint(1, 100), 'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Step 3: Create RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(svm_model, param_distributions=param_dist, n_iter=20, cv=5)\n",
    "\n",
    "# Fit the model with randomized search\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters (Random):\", random_search.best_params_)\n",
    "print(\"Best Score (Random):\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273dd64a",
   "metadata": {},
   "source": [
    "## 5. Putting It All Together: Hands-On Activity\n",
    "Now let's apply all steps together:\n",
    "\n",
    "1. Train a Model: Train an SVM, Decision Tree, or any other model using a chosen dataset (e.g., breast cancer or Iris dataset).\n",
    "\n",
    "\n",
    "2. Evaluate the Model: Use accuracy, precision, recall, F1-score, and confusion matrix to evaluate model performance.\n",
    "\n",
    "\n",
    "3. Cross-Validation: Implement k-fold cross-validation to assess model performance more reliably.\n",
    "\n",
    "\n",
    "4. Tune Hyperparameters: Use GridSearchCV or RandomizedSearchCV to find the optimal hyperparameters for the model.\n",
    "\n",
    "\n",
    "5. Compare Results: Compare the model's performance before and after hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c210b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary\n",
    "- Model training involves data preparation, algorithm selection, training, and evaluation.\n",
    "- Evaluation metrics help us understand model performance better.\n",
    "- Cross-validation provides better generalization.\n",
    "- Hyperparameter tuning optimizes model performance.\n",
    "\n",
    "âœ… Practice Tip: Try using different models like `DecisionTreeClassifier`, `RandomForestClassifier` and compare their performance too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
